---
layout: post
title: "[机器学习] 1 - 从 MNIST 入门 CNN"
subtitle: 以及 pytorch 的一些基本用法
author: "Dongbo"
header-style: text
mathjax: true
hidden: false
tags:
  - 
---

> 其实本来是要做一些强化学习的学习笔记，但是因为似乎需要对比一下DQN等深度强化学习算法，而我本身对于深度学习认知可以说仅停留在能将各种网络名称缩写映射到全称的水平上，所以还是需要从头开始了解一下。
>
> 可能不久之后就会记不得了吧，但是不要紧，我曾尝试过。


1. 卷积层：卷积核的设置，输入特征维度的选择，还有卷积层的定义是什么？
2. ReLU：线性整流函数，作用，类别
3. 全连接层：dropout的作用？ 说到底，整个全连接层的作用是什么？
4. 损失函数：交叉熵函数，以及其他损失函数区别，优缺点，选用理由
5. Adam优化：是什么？
6. 另外，forward 一般是干什么的来着；什么是反向传播来着？
7. optimizer 到底怎么回事？



unsqueeze(0) 在指定的维度上包装上一个维度为1的
squeeze 是去掉维度为1的

view 


loss 尼玛的一直上升，怎么排查？


代码
马尔可夫奖励过程的计算
1. 蒙特卡罗方法
2. DP
3. TD learning（想看看


有空看看这些公式的原始定义，别老惦记着知乎了

### 一些损失函数

来自 https://zhuanlan.zhihu.com/p/83131026

- l1Loss

$$
loss(x,y) = \frac{1}{n} \sum_{i=1}^{n}|y_i - f(x_i)|
$$

- l2Loss 
$$
loss(x,y) = \frac{1}{n} \sum_{i=1}^{n}|y_i - f(x_i)|^2
$$

- SmoothL1Loss
<!-- \begin{equation} 之后会有公式的标号 -->
$$
loss(x,y) = \frac{1}{n} 
\left\{
  \begin{array}{lr}
  5 \times (y_i - f(x_i))^2, & if |y - f(x_i)| < 1\\
  |y - f(x_i)| - 0.5, & otherwise
  \end{array}
\right.
$$

是结合了 L1 和 L2，损失函数比较圆滑，梯度值小，比较稳定，不容易梯度爆炸

- CrossEntropyLoss 交叉熵损失函数

$$
loss(x,y) = - \frac{1}{n} \sum_{i=1}^n \log_3 q(x_i) 
$$
这是估计的公式，好像也有前面的 1/n 是用概率的

### 激活函数

激活函数是为了：增加非线性因素？
- ReLU

$$
ReLU(x) = 
\left\{
  \begin{array}{lr}
  0, & x \le 0 \\
  x, & x > 0
  \end{array}
\right.
$$
