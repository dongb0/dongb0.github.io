---
layout: post
title: "[]"
subtitle: 
author: "Dongbo"
header-style: text
mathjax: true
hidden: false
tags:
  - 
---

2006 UCT Bandit based Monte-Carlo Planning

abstract
对于大状态空间下的马尔可夫决策过程，Monte-Carlo方法（planning）是为数不多能找到近似最优解的方法；

这里提出一种新算法 UCT 来应用 bandit 方法指导MC planning.

1 Introduction

现下（2006年奥）许多使用 Monte-Carlo 搜索的 游戏算法，都是采用 uniform sampling 统一抽样 或者一些启发式的概率选择方法
对选择结果没有保证（guarantee

所以这里对算法的要求是：能够平衡当前最优解的利用，和对次优选择的探索，

本文基于 UCB，提出的算法 UCT (UCB applied to Trees)


2 The UCT Algorithm

2.1 Rollout-based Planning


是缓存历史 state-action-reward 的意思吗？



2.2  Stochastic bandit problem and UCB1

描述了随机老虎机问题，

多臂老虎机方法得到的解，与最优解之间的差别就是”花费在非最优选择分支“上的操作。

该问题就是要平衡 exploration-exploitation tradeoff.

// argmax 是不是就是max？

2.3 The proposed Algorithm
