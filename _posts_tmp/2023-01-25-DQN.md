---
layout: post
title: "[]"
subtitle: 
author: "Dongbo"
header-style: text
mathjax: true
hidden: false
tags:
  - 
---

abstract 
使用CNN结合 Q-learning，使用原始图像数据作为输入，来训练模型来玩 atari 游戏，并且在实验的 7 个游戏中的6个都优于以往的方法，并且在其中3个游戏的成绩还超过了人类。


1 Intro
传统 RL 方法很依赖特征的提取（表征？质量 // 这在以往需要人工构造

而深度学习使得从原始（图像/视频）数据中提取 high-level 特征 成为可能。

但是，应用深度学习在 RL 上面临挑战：
1）深度学习模型需要大量手工标注的数据；RL从信号中得到的reward,可能是很长一段时间前signal带来的
2）深度学习算法假设数据样本是独立的，但RL的通常是关联的；另外RL学习新行动之后，数据的分布可能发生改变，这与深度学习假设数据分布是固定的会发生冲突

// 那么这篇论文是如何解决这一问题的呢
Q-learning 变种，通过随机梯度下降（stochastic gradient descent，SGD）更新权重

并通过 experience replay mechanism 来缓解数据关联和非固定分布的问题

不使用人工标注的数据，完全由视频输入、reward、终止信号、possible action来学习

2 Background

// 什么是 off-policy learning？


// 说实话看完之后还是不太清楚 模型到底是怎么样的，我只知道是用 CNN 替换了 Q-learning 中的 Q表；以及 CNN是两个隐藏层和一个输入层，接收4帧图像作为输入；

用了经验回放，但什么是经验回放？有一个 memory pool？什么时候取经验值？什么时候放？放的内容是什麽？

// DQN 似乎是 on-policy 的，但是怎么确认？



